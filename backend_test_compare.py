#!/usr/bin/env python3
"""
Backend API Testing for NEW Comparative Results Features
Focus: Testing the new /api/llm/compare-performance endpoint and related functionality
"""

import requests
import json
import sys
import time
from datetime import datetime
from typing import Dict, Any

class ComparativeResultsTester:
    def __init__(self, base_url: str = "https://pdf-evolution.preview.emergentagent.com"):
        self.base_url = base_url
        self.api_url = f"{base_url}/api"
        self.tests_run = 0
        self.tests_passed = 0
        self.test_results = []
        
        # Mock data for testing
        self.mock_run_id = "test-run-12345"
        self.mock_study_ids = ["study-1", "study-2"]

    def log_test(self, test_name: str, success: bool, details: str, response_data: Any = None):
        """Log test result"""
        self.tests_run += 1
        if success:
            self.tests_passed += 1
            status = "âœ… PASS"
        else:
            status = "âŒ FAIL"
        
        result = {
            "test": test_name,
            "status": status,
            "success": success,
            "details": details,
            "response_data": response_data,
            "timestamp": datetime.now().isoformat()
        }
        self.test_results.append(result)
        print(f"{status} {test_name}: {details}")

    def test_api_health(self):
        """Test basic API connectivity"""
        try:
            response = requests.get(f"{self.base_url}/", timeout=10)
            if response.status_code == 200:
                self.log_test("API Health Check", True, f"Status: {response.status_code}")
                return True
            else:
                self.log_test("API Health Check", False, f"Status: {response.status_code}")
                return False
        except Exception as e:
            self.log_test("API Health Check", False, f"Error: {str(e)}")
            return False

    def test_compare_performance_endpoint(self):
        """Test the new /api/llm/compare-performance endpoint"""
        try:
            # Test with mock data since we need a run_id and study_ids
            payload = {
                "runId": self.mock_run_id,
                "studyIds": self.mock_study_ids,
                "context": "Compare DarWinSymbiont simulation performance with original study results"
            }
            
            response = requests.post(
                f"{self.api_url}/llm/compare-performance",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=60  # Longer timeout for LLM calls
            )
            
            if response.status_code == 404:
                # Expected since we're using mock run_id
                self.log_test("Compare Performance Endpoint Structure", True, 
                            "Endpoint exists and properly validates run_id (404 for non-existent run)")
                return True
            elif response.status_code == 200:
                data = response.json()
                # Check if response has expected structure
                expected_keys = ["verdict", "summary", "comparisonTable", "dwsStrengths", "studyLimitations"]
                has_structure = any(key in data for key in expected_keys)
                
                if has_structure:
                    self.log_test("Compare Performance Endpoint", True, 
                                "Endpoint working, returned comparison data", data)
                    return True
                else:
                    self.log_test("Compare Performance Endpoint", False, 
                                f"Unexpected response structure: {data}")
                    return False
            else:
                self.log_test("Compare Performance Endpoint", False, 
                            f"Status: {response.status_code}, Response: {response.text}")
                return False
                
        except Exception as e:
            self.log_test("Compare Performance Endpoint", False, f"Error: {str(e)}")
            return False

    def test_latex_with_comparison(self):
        """Test LaTeX generation with comparative analysis"""
        try:
            payload = {
                "runId": self.mock_run_id,
                "studyIds": self.mock_study_ids,
                "comparison": {
                    "verdict": "outperformed",
                    "summary": "Test comparison summary",
                    "comparisonTable": [
                        {"metric": "Performance", "studyResult": "0.75", "dwsResult": "0.85"}
                    ]
                },
                "context": "Generated from DarWinSymbiont evolutionary simulation results with comparative analysis"
            }
            
            response = requests.post(
                f"{self.api_url}/llm/latex",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=60
            )
            
            if response.status_code == 404:
                self.log_test("LaTeX with Comparison", True, 
                            "Endpoint exists and properly validates run_id (404 for non-existent run)")
                return True
            elif response.status_code == 200:
                data = response.json()
                if "latex" in data:
                    # Check if LaTeX contains comparative analysis section
                    latex_content = data["latex"].lower()
                    has_comparison = "comparative" in latex_content or "comparison" in latex_content
                    
                    self.log_test("LaTeX with Comparison", has_comparison, 
                                f"LaTeX generated {'with' if has_comparison else 'without'} comparative analysis section")
                    return has_comparison
                else:
                    self.log_test("LaTeX with Comparison", False, f"No LaTeX in response: {data}")
                    return False
            else:
                self.log_test("LaTeX with Comparison", False, 
                            f"Status: {response.status_code}, Response: {response.text}")
                return False
                
        except Exception as e:
            self.log_test("LaTeX with Comparison", False, f"Error: {str(e)}")
            return False

    def test_studies_endpoint(self):
        """Test studies endpoint to check if we have any existing data"""
        try:
            response = requests.get(f"{self.api_url}/studies", timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                studies = data.get("studies", [])
                self.log_test("Studies Endpoint", True, 
                            f"Found {len(studies)} studies in database", 
                            {"study_count": len(studies)})
                return studies
            else:
                self.log_test("Studies Endpoint", False, 
                            f"Status: {response.status_code}, Response: {response.text}")
                return []
                
        except Exception as e:
            self.log_test("Studies Endpoint", False, f"Error: {str(e)}")
            return []

    def run_all_tests(self):
        """Run all comparative results tests"""
        print("ðŸ§ª Testing NEW Comparative Results Features")
        print("=" * 60)
        
        # Basic connectivity
        if not self.test_api_health():
            print("âŒ API not accessible, stopping tests")
            return self.generate_report()
        
        # Check existing data
        studies = self.test_studies_endpoint()
        
        # Test new compare-performance endpoint
        self.test_compare_performance_endpoint()
        
        # Test LaTeX generation with comparison
        self.test_latex_with_comparison()
        
        return self.generate_report()

    def generate_report(self):
        """Generate test report"""
        success_rate = (self.tests_passed / self.tests_run * 100) if self.tests_run > 0 else 0
        
        report = {
            "summary": {
                "total_tests": self.tests_run,
                "passed_tests": self.tests_passed,
                "success_rate": success_rate,
                "timestamp": datetime.now().isoformat(),
                "focus": "NEW Comparative Results Features Testing"
            },
            "test_results": self.test_results
        }
        
        print(f"\nðŸ“Š Test Summary:")
        print(f"Tests Run: {self.tests_run}")
        print(f"Tests Passed: {self.tests_passed}")
        print(f"Success Rate: {success_rate:.1f}%")
        
        # Save report
        with open("/app/test_reports/comparative_results_api_test.json", "w") as f:
            json.dump(report, f, indent=2)
        
        return report

if __name__ == "__main__":
    tester = ComparativeResultsTester()
    report = tester.run_all_tests()
    
    # Exit with appropriate code
    sys.exit(0 if report["summary"]["success_rate"] > 50 else 1)